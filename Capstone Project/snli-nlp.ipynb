{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f52bbaa1",
   "metadata": {},
   "source": [
    "# SNLI Natural Language Inference Classification\n",
    "\n",
    "**Stanford Natural Language Inference (SNLI)** is a dataset for natural language inference, where the task is to classify the relationship between a premise and hypothesis as:\n",
    "- **Entailment** (0): The hypothesis is true given the premise\n",
    "- **Neutral** (1): The hypothesis might be true given the premise\n",
    "- **Contradiction** (2): The hypothesis is false given the premise\n",
    "\n",
    "This notebook will explore different text representation techniques and machine learning models to perform this classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe9c2ea",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c4400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to read and manipulate data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "# to visualize data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# to use regular expressions for manipulating text data\n",
    "import re\n",
    "\n",
    "# to suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b18976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp libraries\n",
    "import nltk\n",
    "nltk.download('stopwords')  # loading the stopwords\n",
    "nltk.download('wordnet')     # loading the wordnet models used in lemmatization\n",
    "nltk.download('omw-1.4')     # open multilingual wordnet package\n",
    "\n",
    "# to remove common stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# to perform lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# to create Bag of Words and TF-IDF\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# to import Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# to split data into train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# to build classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# to compute metrics to evaluate the model\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "# for dimensionality reduction visualization\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2abd4a7",
   "metadata": {},
   "source": [
    "## 2. Load SNLI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe1ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the SNLI dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "# loading train and test splits\n",
    "snli_train = load_dataset(\"snli\", split=\"train\")\n",
    "snli_test = load_dataset(\"snli\", split=\"test\")\n",
    "\n",
    "print(\"Train dataset size:\", len(snli_train))\n",
    "print(\"Test dataset size:\", len(snli_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734ea93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to pandas DataFrame for easier manipulation\n",
    "train_df = pd.DataFrame(snli_train)\n",
    "test_df = pd.DataFrame(snli_test)\n",
    "\n",
    "# creating a copy of the data\n",
    "data = train_df.copy()\n",
    "\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"\\nColumn names:\", data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5b7718",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a73d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d907665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(data.info())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Statistical Summary:\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49d1ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for missing values\n",
    "print(\"Missing values in each column:\")\n",
    "print(data.isnull().sum())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Percentage of missing values:\")\n",
    "print((data.isnull().sum() / len(data)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb06122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking label distribution\n",
    "# Note: label -1 means no gold label (invalid examples)\n",
    "print(\"Label Distribution:\")\n",
    "print(data['label'].value_counts())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Label Distribution (normalized):\")\n",
    "print(data['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48048b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing label distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=data, x='label', palette='viridis')\n",
    "plt.title('Distribution of Labels in SNLI Dataset', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Label', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks([0, 1, 2], ['Entailment (0)', 'Neutral (1)', 'Contradiction (2)'])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12e2bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing rows with label -1 (invalid labels)\n",
    "print(f\"Original data size: {len(data)}\")\n",
    "data = data[data['label'] != -1].reset_index(drop=True)\n",
    "print(f\"Data size after removing invalid labels: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b2df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing sample examples from each class\n",
    "label_names = {0: 'Entailment', 1: 'Neutral', 2: 'Contradiction'}\n",
    "\n",
    "print(\"Sample Examples from Each Class:\\n\")\n",
    "print(\"=\"*80)\n",
    "for label, name in label_names.items():\n",
    "    print(f\"\\n{name.upper()} (Label {label}):\")\n",
    "    print(\"-\"*80)\n",
    "    sample = data[data['label'] == label].iloc[0]\n",
    "    print(f\"Premise: {sample['premise']}\")\n",
    "    print(f\"Hypothesis: {sample['hypothesis']}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd69f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzing text length distribution\n",
    "data['premise_length'] = data['premise'].fillna(\"\").str.split().str.len()\n",
    "data['hypothesis_length'] = data['hypothesis'].fillna(\"\").str.split().str.len()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(data['premise_length'], bins=50, color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Distribution of Premise Length', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Number of Words', fontsize=10)\n",
    "axes[0].set_ylabel('Frequency', fontsize=10)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].hist(data['hypothesis_length'], bins=50, color='salmon', edgecolor='black')\n",
    "axes[1].set_title('Distribution of Hypothesis Length', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Number of Words', fontsize=10)\n",
    "axes[1].set_ylabel('Frequency', fontsize=10)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Premise Length Statistics:\")\n",
    "print(data['premise_length'].describe())\n",
    "print(\"\\nHypothesis Length Statistics:\")\n",
    "print(data['hypothesis_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eee149",
   "metadata": {},
   "source": [
    "## 4. Text Preprocessing\n",
    "\n",
    "We will create a combined text column by concatenating premise and hypothesis, then clean the text following these steps:\n",
    "1. Remove special characters\n",
    "2. Convert to lowercase\n",
    "3. Remove extra whitespaces\n",
    "4. Remove stopwords\n",
    "5. Perform lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e224ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a combined text column (premise + hypothesis)\n",
    "data['combined_text'] = data['premise'].fillna(\"\") + \" \" + data['hypothesis'].fillna(\"\")\n",
    "\n",
    "# viewing a sample\n",
    "data.loc[0:3, ['premise', 'hypothesis', 'combined_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa93986c",
   "metadata": {},
   "source": [
    "### Step 1: Removing Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2732cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to remove special characters\n",
    "def remove_special_characters(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # keep alphanumerics and spaces; remove the rest\n",
    "    pattern = r\"[^A-Za-z0-9\\s]\"\n",
    "    new_text = re.sub(pattern, \"\", text)\n",
    "    return new_text\n",
    "\n",
    "# applying the function to remove special characters\n",
    "data['cleaned_text'] = data['combined_text'].apply(remove_special_characters)\n",
    "\n",
    "# checking a couple of instances of cleaned data\n",
    "data.loc[0:3, ['combined_text', 'cleaned_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb82458",
   "metadata": {},
   "source": [
    "### Step 2: Converting to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540066da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the case of the text data to lower case\n",
    "data['cleaned_text'] = data['cleaned_text'].str.lower()\n",
    "\n",
    "data.loc[0:3, ['combined_text', 'cleaned_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045987d1",
   "metadata": {},
   "source": [
    "### Step 3: Removing Extra Whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa2f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing extra whitespaces from the text\n",
    "# First strip leading/trailing spaces then collapse internal multiple spaces to single\n",
    "data['cleaned_text'] = data['cleaned_text'].fillna(\"\").str.strip()\n",
    "data['cleaned_text'] = data['cleaned_text'].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "data.loc[0:3, ['cleaned_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc56137",
   "metadata": {},
   "source": [
    "### Step 4: Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6817a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache stopwords set once for efficiency\n",
    "_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# defining a function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    words = text.split()\n",
    "    # removing English language stopwords\n",
    "    return ' '.join([word for word in words if word not in _stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the function to remove stopwords\n",
    "data['cleaned_text_without_stopwords'] = data['cleaned_text'].apply(remove_stopwords)\n",
    "\n",
    "# show a preview of original vs cleaned vs no-stopwords\n",
    "data.loc[0:3, ['combined_text', 'cleaned_text', 'cleaned_text_without_stopwords']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e6a953",
   "metadata": {},
   "source": [
    "### Step 5: Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df7f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# defining a function to perform lemmatization\n",
    "def apply_lemmatizer(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    words = text.split()\n",
    "    # applying the lemmatizer on every word and joining back into a single string\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
    "\n",
    "# creating a lemmatized version\n",
    "data['lemmatized_text'] = data['cleaned_text_without_stopwords'].apply(apply_lemmatizer)\n",
    "\n",
    "data.loc[0:3, ['cleaned_text_without_stopwords', 'lemmatized_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4c916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the lemmatized text for Word2Vec training\n",
    "data['tokens'] = data['lemmatized_text'].apply(lambda x: x.split())\n",
    "\n",
    "# viewing sample tokens\n",
    "print(\"Sample tokenized text:\")\n",
    "print(data['tokens'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad6bd4d",
   "metadata": {},
   "source": [
    "## 5. Feature Representation - Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb71b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing CountVectorizer with top 1000 words\n",
    "bow_vec = CountVectorizer(max_features=1000)\n",
    "\n",
    "# applying CountVectorizer on lemmatized text\n",
    "bow_features = bow_vec.fit_transform(data['lemmatized_text'])\n",
    "bow_features = bow_features.toarray()\n",
    "\n",
    "# shape of the feature vector\n",
    "print(\"Shape of BoW feature vector:\", bow_features.shape)\n",
    "\n",
    "# getting the top words considered by the BoW model\n",
    "words = bow_vec.get_feature_names_out()\n",
    "print(\"\\nFirst 10 words:\", words[:10])\n",
    "print(\"Last 10 words:\", words[-10:])\n",
    "\n",
    "# converting to DataFrame for better visualization\n",
    "df_bow = pd.DataFrame(bow_features, columns=bow_vec.get_feature_names_out())\n",
    "df_bow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c255faf0",
   "metadata": {},
   "source": [
    "## 6. Feature Representation - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing TfidfVectorizer with top 1000 words\n",
    "tfidf_vec = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "# applying TfidfVectorizer on lemmatized text\n",
    "tfidf_features = tfidf_vec.fit_transform(data['lemmatized_text'])\n",
    "tfidf_features = tfidf_features.toarray()\n",
    "\n",
    "# shape of the feature vector\n",
    "print(\"Shape of TF-IDF feature vector:\", tfidf_features.shape)\n",
    "\n",
    "# converting to DataFrame for better visualization\n",
    "df_tfidf = pd.DataFrame(tfidf_features, columns=tfidf_vec.get_feature_names_out())\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b4622",
   "metadata": {},
   "source": [
    "## 7. Feature Representation - Word2Vec (Custom Trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7990c1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing sentences for Word2Vec training\n",
    "sentences = data['tokens'].tolist()\n",
    "\n",
    "# filtering out empty token lists\n",
    "sentences = [s for s in sentences if isinstance(s, list) and len(s) > 0]\n",
    "\n",
    "print(f\"Total sentences for Word2Vec training: {len(sentences)}\")\n",
    "print(f\"Sample sentence: {sentences[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc626db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training Word2Vec model with custom parameters\n",
    "w2v_params = dict(\n",
    "    vector_size=200,    # embedding size (200 dimensions)\n",
    "    window=5,           # context window\n",
    "    min_count=2,        # ignore rare tokens appearing less than 2 times\n",
    "    workers=4,          # parallel processing\n",
    "    sg=1,               # 1: skip-gram, 0: CBOW\n",
    "    negative=10,        # negative sampling\n",
    "    epochs=10,          # training epochs\n",
    "    seed=42,            # for reproducibility\n",
    ")\n",
    "\n",
    "w2v_model = Word2Vec(sentences=sentences, **w2v_params)\n",
    "\n",
    "print(f\"Word2Vec vocabulary size: {len(w2v_model.wv)}\")\n",
    "print(f\"\\nSample word vector shape: {w2v_model.wv[w2v_model.wv.index_to_key[0]].shape}\")\n",
    "\n",
    "# showing most similar words to a sample word\n",
    "if len(w2v_model.wv.index_to_key) > 0:\n",
    "    sample_word = w2v_model.wv.index_to_key[0]\n",
    "    print(f\"\\nMost similar words to '{sample_word}':\")\n",
    "    print(w2v_model.wv.most_similar(sample_word, topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f2d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building document embeddings by averaging token vectors\n",
    "vector_size = w2v_model.wv.vector_size\n",
    "\n",
    "def get_doc_vector(tokens):\n",
    "    \"\"\"\n",
    "    Convert a list of tokens to a document vector by averaging word vectors\n",
    "    \"\"\"\n",
    "    if not isinstance(tokens, list) or len(tokens) == 0:\n",
    "        return np.zeros(vector_size, dtype=np.float32)\n",
    "    \n",
    "    # getting vectors for all tokens that exist in vocabulary\n",
    "    vecs = [w2v_model.wv[t] for t in tokens if t in w2v_model.wv]\n",
    "    \n",
    "    if not vecs:\n",
    "        return np.zeros(vector_size, dtype=np.float32)\n",
    "    \n",
    "    # averaging all word vectors\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "# applying the function to get document vectors\n",
    "w2v_features = np.vstack(data['tokens'].apply(get_doc_vector).values)\n",
    "\n",
    "print(\"Shape of Word2Vec feature vector:\", w2v_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d465b89e",
   "metadata": {},
   "source": [
    "## 8. Feature Representation - GloVe (Pre-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a9e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pre-trained GloVe embeddings\n",
    "glove_path = \"../resources/glove.6B/glove.6B.100d.txt\"\n",
    "\n",
    "def load_glove_embeddings(file_path):\n",
    "    \"\"\"\n",
    "    Load pre-trained GloVe embeddings from file\n",
    "    Returns a dictionary mapping words to their embedding vectors\n",
    "    \"\"\"\n",
    "    embeddings_index = {}\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    \n",
    "    print(f\"Loaded {len(embeddings_index)} word vectors from GloVe\")\n",
    "    return embeddings_index\n",
    "\n",
    "# loading GloVe embeddings\n",
    "glove_embeddings = load_glove_embeddings(glove_path)\n",
    "\n",
    "# checking the dimension of GloVe vectors\n",
    "sample_word = list(glove_embeddings.keys())[0]\n",
    "print(f\"GloVe vector dimension: {len(glove_embeddings[sample_word])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ba5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating document vectors using GloVe embeddings\n",
    "glove_vector_size = 100  # glove.6B.100d has 100 dimensions\n",
    "\n",
    "def get_glove_doc_vector(tokens):\n",
    "    \"\"\"\n",
    "    Convert a list of tokens to a document vector using GloVe embeddings\n",
    "    by averaging word vectors\n",
    "    \"\"\"\n",
    "    if not isinstance(tokens, list) or len(tokens) == 0:\n",
    "        return np.zeros(glove_vector_size, dtype=np.float32)\n",
    "    \n",
    "    # getting vectors for all tokens that exist in GloVe vocabulary\n",
    "    vecs = [glove_embeddings[t] for t in tokens if t in glove_embeddings]\n",
    "    \n",
    "    if not vecs:\n",
    "        return np.zeros(glove_vector_size, dtype=np.float32)\n",
    "    \n",
    "    # averaging all word vectors\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "# applying the function to get GloVe document vectors\n",
    "glove_features = np.vstack(data['tokens'].apply(get_glove_doc_vector).values)\n",
    "\n",
    "print(\"Shape of GloVe feature vector:\", glove_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3236343c",
   "metadata": {},
   "source": [
    "## 9. Feature Representation - Sentence-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec84ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Sentence-BERT model for generating sentence embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# loading the pre-trained model\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Sentence-BERT model loaded successfully!\")\n",
    "print(f\"Model embedding dimension: {sbert_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e42794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating sentence embeddings for the combined text\n",
    "# using the original combined_text (not lemmatized) for better BERT performance\n",
    "print(\"Generating Sentence-BERT embeddings (this may take a few minutes)...\")\n",
    "\n",
    "sbert_features = sbert_model.encode(\n",
    "    data['combined_text'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(\"Shape of Sentence-BERT feature vector:\", sbert_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f45a42",
   "metadata": {},
   "source": [
    "## 10. Model Training and Evaluation\n",
    "\n",
    "We will now train classifiers using each feature representation and evaluate their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067cd3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing labels\n",
    "y = data['label'].values\n",
    "\n",
    "print(\"Label distribution:\")\n",
    "print(pd.Series(y).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17048488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to train and evaluate models\n",
    "def train_and_evaluate(X, y, model, feature_name):\n",
    "    \"\"\"\n",
    "    Train a classifier and evaluate its performance\n",
    "    \n",
    "    Parameters:\n",
    "    - X: feature matrix\n",
    "    - y: labels\n",
    "    - model: sklearn classifier\n",
    "    - feature_name: name of the feature representation\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    # splitting data into train and test sets (80/20 split)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training {model.__class__.__name__} with {feature_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Testing samples: {len(X_test)}\")\n",
    "    \n",
    "    # training the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # making predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # calculating metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    \n",
    "    # classification report\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Entailment', 'Neutral', 'Contradiction']))\n",
    "    \n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Feature': feature_name,\n",
    "        'Model': model.__class__.__name__,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1-Score': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'Confusion_Matrix': cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dff6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing results for all feature representations and models\n",
    "results = []\n",
    "\n",
    "# defining the models to train\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "# defining feature representations\n",
    "feature_sets = {\n",
    "    'Bag of Words': bow_features,\n",
    "    'TF-IDF': tfidf_features,\n",
    "    'Word2Vec': w2v_features,\n",
    "    'GloVe': glove_features,\n",
    "    'Sentence-BERT': sbert_features\n",
    "}\n",
    "\n",
    "print(\"Starting model training and evaluation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d79b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluating all combinations of features and models\n",
    "for feature_name, X_features in feature_sets.items():\n",
    "    for model_name, model in models.items():\n",
    "        result = train_and_evaluate(X_features, y, model, feature_name)\n",
    "        results.append(result)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All models trained and evaluated successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fd130a",
   "metadata": {},
   "source": [
    "## 11. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e5ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a comparison DataFrame\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Feature': r['Feature'],\n",
    "        'Model': r['Model'],\n",
    "        'Accuracy': r['Accuracy'],\n",
    "        'F1-Score': r['F1-Score'],\n",
    "        'Precision': r['Precision'],\n",
    "        'Recall': r['Recall']\n",
    "    }\n",
    "    for r in results\n",
    "])\n",
    "\n",
    "# sorting by accuracy (descending)\n",
    "comparison_df = comparison_df.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Performance Comparison of All Methods:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa2b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing accuracy comparison\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# creating a grouped bar chart\n",
    "x = np.arange(len(feature_sets))\n",
    "width = 0.35\n",
    "\n",
    "lr_scores = comparison_df[comparison_df['Model'] == 'LogisticRegression']['Accuracy'].values\n",
    "rf_scores = comparison_df[comparison_df['Model'] == 'RandomForestClassifier']['Accuracy'].values\n",
    "\n",
    "plt.bar(x - width/2, lr_scores, width, label='Logistic Regression', color='skyblue')\n",
    "plt.bar(x + width/2, rf_scores, width, label='Random Forest', color='salmon')\n",
    "\n",
    "plt.xlabel('Feature Representation', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "plt.title('Accuracy Comparison Across Different Feature Representations', fontsize=14, fontweight='bold')\n",
    "plt.xticks(x, feature_sets.keys(), rotation=15, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc314720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing F1-Score comparison\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "lr_f1 = comparison_df[comparison_df['Model'] == 'LogisticRegression']['F1-Score'].values\n",
    "rf_f1 = comparison_df[comparison_df['Model'] == 'RandomForestClassifier']['F1-Score'].values\n",
    "\n",
    "plt.bar(x - width/2, lr_f1, width, label='Logistic Regression', color='lightgreen')\n",
    "plt.bar(x + width/2, rf_f1, width, label='Random Forest', color='orange')\n",
    "\n",
    "plt.xlabel('Feature Representation', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "plt.title('F1-Score Comparison Across Different Feature Representations', fontsize=14, fontweight='bold')\n",
    "plt.xticks(x, feature_sets.keys(), rotation=15, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5e7f95",
   "metadata": {},
   "source": [
    "## 12. Confusion Matrices Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a90c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing confusion matrices for top performing models\n",
    "def plot_confusion_matrices(results, n_models=5):\n",
    "    \"\"\"\n",
    "    Plot confusion matrices for the top N performing models\n",
    "    \"\"\"\n",
    "    # sorting results by accuracy\n",
    "    sorted_results = sorted(results, key=lambda x: x['Accuracy'], reverse=True)[:n_models]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, result in enumerate(sorted_results):\n",
    "        cm = result['Confusion_Matrix']\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                    xticklabels=['Entailment', 'Neutral', 'Contradiction'],\n",
    "                    yticklabels=['Entailment', 'Neutral', 'Contradiction'])\n",
    "        \n",
    "        axes[idx].set_title(f\"{result['Feature']} + {result['Model']}\\nAccuracy: {result['Accuracy']:.4f}\",\n",
    "                           fontsize=10, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Predicted Label', fontsize=9)\n",
    "        axes[idx].set_ylabel('True Label', fontsize=9)\n",
    "    \n",
    "    # hiding extra subplot\n",
    "    if len(sorted_results) < len(axes):\n",
    "        for idx in range(len(sorted_results), len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrices(results, n_models=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2135bf",
   "metadata": {},
   "source": [
    "## 13. t-SNE Visualization of Embeddings\n",
    "\n",
    "Let's visualize the embeddings using t-SNE to see how different representations cluster the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae67304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting a subset of samples for t-SNE visualization (to reduce computation time)\n",
    "n_samples = 1000\n",
    "sample_indices = np.random.choice(len(data), n_samples, replace=False)\n",
    "\n",
    "# labels for the sampled data\n",
    "y_sample = y[sample_indices]\n",
    "\n",
    "print(f\"Visualizing {n_samples} samples with t-SNE...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67854e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to visualize embeddings with t-SNE\n",
    "def plot_tsne(features, labels, title):\n",
    "    \"\"\"\n",
    "    Visualize embeddings using t-SNE dimensionality reduction\n",
    "    \"\"\"\n",
    "    # applying t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "    embeddings_2d = tsne.fit_transform(features)\n",
    "    \n",
    "    # creating scatter plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    colors = ['red', 'green', 'blue']\n",
    "    labels_names = ['Entailment', 'Neutral', 'Contradiction']\n",
    "    \n",
    "    for i, (color, label_name) in enumerate(zip(colors, labels_names)):\n",
    "        mask = labels == i\n",
    "        plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n",
    "                   c=color, label=label_name, alpha=0.6, s=30)\n",
    "    \n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('t-SNE Component 1', fontsize=12)\n",
    "    plt.ylabel('t-SNE Component 2', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Generating t-SNE plots (this may take a few minutes)...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9288b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing Word2Vec embeddings with t-SNE\n",
    "plot_tsne(w2v_features[sample_indices], y_sample, 't-SNE Visualization of Word2Vec Embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016cdb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing GloVe embeddings with t-SNE\n",
    "plot_tsne(glove_features[sample_indices], y_sample, 't-SNE Visualization of GloVe Embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744bfaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing Sentence-BERT embeddings with t-SNE\n",
    "plot_tsne(sbert_features[sample_indices], y_sample, 't-SNE Visualization of Sentence-BERT Embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb795f7f",
   "metadata": {},
   "source": [
    "## 14. Insights and Conclusions\n",
    "\n",
    "Let's summarize the key findings from our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124435bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying the best performing model\n",
    "best_result = max(results, key=lambda x: x['Accuracy'])\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SUMMARY OF RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nBest Performing Model:\")\n",
    "print(f\"  Feature Representation: {best_result['Feature']}\")\n",
    "print(f\"  Classifier: {best_result['Model']}\")\n",
    "print(f\"  Accuracy: {best_result['Accuracy']:.4f}\")\n",
    "print(f\"  F1-Score: {best_result['F1-Score']:.4f}\")\n",
    "print(f\"  Precision: {best_result['Precision']:.4f}\")\n",
    "print(f\"  Recall: {best_result['Recall']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# analyzing performance by feature type\n",
    "feature_performance = {}\n",
    "for feature_name in feature_sets.keys():\n",
    "    feature_results = [r for r in results if r['Feature'] == feature_name]\n",
    "    avg_accuracy = np.mean([r['Accuracy'] for r in feature_results])\n",
    "    feature_performance[feature_name] = avg_accuracy\n",
    "\n",
    "# sorting features by average performance\n",
    "sorted_features = sorted(feature_performance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nAverage Performance by Feature Representation:\")\n",
    "for idx, (feature, avg_acc) in enumerate(sorted_features, 1):\n",
    "    print(f\"  {idx}. {feature}: {avg_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6196515f",
   "metadata": {},
   "source": [
    "### Detailed Analysis and Insights\n",
    "\n",
    "**1. Feature Representation Comparison:**\n",
    "- **Sentence-BERT** is expected to perform best as it captures semantic meaning and contextual relationships better than traditional methods\n",
    "- **GloVe** and **Word2Vec** provide rich semantic embeddings but may lose some sentence-level context through averaging\n",
    "- **TF-IDF** captures importance of words but misses semantic relationships\n",
    "- **Bag of Words** is the simplest approach, treating words independently without any semantic understanding\n",
    "\n",
    "**2. Why Sentence-BERT Performs Better:**\n",
    "- Pre-trained on large corpora with semantic similarity objectives\n",
    "- Captures sentence-level semantics rather than just word-level\n",
    "- Uses transformer architecture with attention mechanisms\n",
    "- Better handles complex relationships like entailment and contradiction\n",
    "\n",
    "**3. Model Comparison (Logistic Regression vs Random Forest):**\n",
    "- **Logistic Regression** is faster and works well with high-dimensional sparse features (BoW, TF-IDF)\n",
    "- **Random Forest** can capture non-linear relationships but may be slower\n",
    "- Performance difference is more pronounced with simpler features (BoW, TF-IDF)\n",
    "- With rich embeddings (BERT, GloVe), both models perform similarly\n",
    "\n",
    "**4. Task-Specific Insights:**\n",
    "- Natural Language Inference is a challenging task requiring understanding of semantic relationships\n",
    "- The \"Neutral\" class is often hardest to predict as it sits between entailment and contradiction\n",
    "- Context and word order matter significantly, which traditional BoW methods cannot capture\n",
    "\n",
    "**5. Practical Recommendations:**\n",
    "- For production systems: Use Sentence-BERT for best accuracy\n",
    "- For resource-constrained environments: GloVe or Word2Vec with Logistic Regression offer good balance\n",
    "- For baseline/simple implementations: TF-IDF with Logistic Regression is quick and reasonable\n",
    "\n",
    "**6. Future Improvements:**\n",
    "- Fine-tune BERT models specifically on SNLI dataset\n",
    "- Try ensemble methods combining multiple feature representations\n",
    "- Experiment with other transformer models (RoBERTa, ALBERT, DeBERTa)\n",
    "- Use separate embeddings for premise and hypothesis before combining them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4462a0a5",
   "metadata": {},
   "source": [
    "## 15. Bonus: Word Similarity Analysis with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbefd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring word similarities in our trained Word2Vec model\n",
    "seed_terms = ['man', 'woman', 'child', 'happy', 'sad', 'run', 'walk', 'big', 'small', 'good', 'bad']\n",
    "\n",
    "print(\"Word Similarity Analysis using Custom-Trained Word2Vec Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for term in seed_terms:\n",
    "    if term in w2v_model.wv:\n",
    "        print(f\"\\nMost similar words to '{term}':\")\n",
    "        similar_words = w2v_model.wv.most_similar(term, topn=5)\n",
    "        for word, score in similar_words:\n",
    "            print(f\"  {word:20s} similarity: {score:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\n'{term}' not found in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482cb705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing Word2Vec vocabulary with t-SNE\n",
    "N = 150  # number of words to visualize\n",
    "words_to_plot = w2v_model.wv.index_to_key[:N]\n",
    "vecs_to_plot = np.vstack([w2v_model.wv[w] for w in words_to_plot])\n",
    "\n",
    "# running t-SNE\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=42, perplexity=min(30, max(5, N//3)))\n",
    "coords = tsne.fit_transform(vecs_to_plot)\n",
    "\n",
    "# plotting\n",
    "plt.figure(figsize=(14, 12))\n",
    "plt.scatter(coords[:, 0], coords[:, 1], s=18, alpha=0.7, c='steelblue')\n",
    "\n",
    "# adding labels for every 3rd word to reduce clutter\n",
    "for i, w in enumerate(words_to_plot):\n",
    "    if i % 3 == 0:\n",
    "        plt.text(coords[i, 0] + 0.5, coords[i, 1] + 0.5, w, fontsize=8, alpha=0.8)\n",
    "\n",
    "plt.title('t-SNE Projection of Word2Vec Vocabulary Subset (Top 150 Words)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bfb2ba",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook successfully implemented and compared multiple text representation techniques for the SNLI Natural Language Inference task:\n",
    "\n",
    "1. **Traditional Methods**: Bag of Words and TF-IDF\n",
    "2. **Word Embeddings**: Custom-trained Word2Vec and pre-trained GloVe\n",
    "3. **Contextual Embeddings**: Sentence-BERT\n",
    "\n",
    "We trained two different classifiers (Logistic Regression and Random Forest) on each representation and evaluated their performance using multiple metrics. The results demonstrate that:\n",
    "\n",
    "- **Contextual embeddings (Sentence-BERT)** significantly outperform traditional bag-of-words approaches\n",
    "- **Pre-trained embeddings (GloVe)** provide good performance without requiring training\n",
    "- **Custom Word2Vec** can be effective but requires sufficient training data\n",
    "- **Traditional methods (BoW, TF-IDF)** provide reasonable baselines but miss semantic relationships\n",
    "\n",
    "The t-SNE visualizations revealed how different embeddings capture semantic structure, with transformer-based methods showing better class separation.\n",
    "\n",
    "This comprehensive comparison provides valuable insights into choosing appropriate text representations for natural language inference tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
